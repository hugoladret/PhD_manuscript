\chapter{Manuscript introduction}
%\addcontentsline{toc}{chapter}{Manuscript introduction}
\begin{flushright}
    \textit{''The journey is the reward.''}\\
    Daoist proverb, attributed to Laozi
\end{flushright}

\section{Overview}
The objective of this thesis is to establish a robust framework for probabilistic computations in vision, encompassing stochastic visual stimulation and advanced data analysis, to enhance our understanding of the neurophysiological mechanisms in \gls{V1} and beyond. We will center primarily on computations related to the variance of visual inputs, which play a fundamental role in our daily life, as highlighted in Chapter 2.

\section{Scientific context}
This work is based on a "cotutelle" (joint) supervision between Laurent Perrinet from the Institut de Neurosciences de la Timone laboratory (INT, Aix-Marseille University \& CNRS), and Christian Casanova, initially based at the Laboratory of Visual Neurosciences (School of Optometry, University of Montreal), now also affiliated with the "\'Ecole de Technologie Sup√©rieure" (Montreal).
This international and interdisciplinary endeavor combines French computational models with Canadian neurophysiological experiments. Over time, this project has also expanded to include several collaborators, such as Nicholas Priebe from the University of Texas at Austin, who conducted preliminary experiments in marmosets, as well as Pieter Roelfsema and Paolo Papale from the Netherlands Institute of Neuroscience, who performed similar recordings in awake behaving macaques.

The interplay between neurobiology and neurocomputations is the keystone of modern neuroscientific research. In line with this, this thesis aims to demonstrate the advantages of a model-driven approach to studying an intricate dynamical system such as the brain, advocating in the process for improved stimuli and generic models. While this manuscript should (hopefully) be accessible for readers with an interdisciplinary background, I have made every effort to ensure that the content is comprehensible regardless of whether the reader's background is more aligned with the silicon or carbon side of the field.

\section{Manuscript organization}
By virtue of the interdisciplinary and international organization of my thesis, this manuscript is longer than most manuscripts have a right to be. Despite this length, a concerted effort has been made to ensure a smooth and engaging reading experience. Rather than beginning with a lengthy introduction aimed at providing a broad overview of the nature of vision and its modern challenges, I instead elected to start with a concise Marr-like approach~\cite{marr1982vision}  to the problem at hand. After this introduction, each chapter is organized as a self-contained read, beginning with a concise introduction regarding the specific sub-problem studied, followed by a related article, and concluded with research perspectives. 

This manuscript is structured into eight sections, as outlined below: 
\begin{itemize} 

\item In the current chapter, \textit{Chapter 1}, the general study is introduced, with the aim of providing a high-level overview of the whole thesis in a comprehensive manner.

\item \textit{Chapter 2} starts the scientific content of the thesis, by introducing the notions on which further chapters are build. Given the multidisciplinary nature of said chapters, this global introduction aims at being a guide for the reader, providing introductory notions of the high-level content of this thesis. The specific details - or the specific experimental methodologies - of each such concepts are then explored in the introductory section of each of the following chapters. \\
By analogy with David Marr's levels of analysis~\cite{marr1982vision}, this introduction begins with the computational nature of perception and vision, then dives into the algorithmic nature of such a perceptual problem, to finally conclude with its implementation at a computational and neurobiological level.

\item \textit{Chapter 3} marks the first of our scientific contributions. It introduces the idea that the images that make up our daily visual experience, so-called "natural images", can be accurately described as a mixture of Gaussian-like distributions in orientation space. In the specific context of the thesis, this serves to support the use of artificial stimuli in the following chapter, in order to deal away with a number of complexity involved in the use of natural images as stimuli in visual experiments~\cite{rust2005praise}. Building upon this concept, we also incorporate such variance in a computational model to demonstrate enhanced performance in orientation encoding and innovative deep learning applications. Specifically, this chapter explores the role of aleatoric (input-bound) and epistemic (model-bound) variance in the encoding of natural images, using a convolutional sparse coding algorithm. The results reveal that the integration of oriented features across multiple levels of epistemic variance significantly boosts the accuracy of sparse coding for natural images. Finally, this chapter also proposes that hierarchical visual processing can benefit from variance encoding, by training a deep convolutional neural network on sparse-coded natural images datasets, and demonstrating that variance-encoded sparse code is not only as effective as conventional images, but also provides a more robust representation of naturalistic images.

\item \textit{Chapter 4} extends on similar Gaussian priors, using a parametric generative model called Motion Clouds~\cite{leon2012motion}. These stimuli allow probing for the neural correlates of variance (in orientation space) using extracellular recordings in the primary visual cortex of anesthetized cats. The chapter quantifies the dynamics of sensory variance modulations of these neurons and reports two novel archetypal neuronal responses, one of which is resilient to changes in variance and co-encodes the sensory feature and its variance, thereby improving the population encoding of orientation. The existence of these variance-specific responses is then accounted for by a computational model of intracortical recurrent connectivity. This chapter's article proposes that local recurrent circuits process variance as a generic computation, advancing our understanding of how the brain handles naturalistic inputs. Arguably, this is the keystone of our contributions during this thesis, and the results obtained here serve as the basis of the remaining chapters.

%\item \textit{Chapter 5} introduces a novel model that explicitly incorporates variance within the well-established predictive coding framework. In contrast to the approach taken in Chapter 5, where input variance was encoded through the replication of features at multiple levels of model variance, we employ here biologically plausible rules to learn the variance of its inputs. This innovative model is applied to tasks of image classification and generation, showcasing the feasibility and benefits of learning variance in image-related tasks. The model's performance reaches state-of-the-art performance, demonstrating a remarkable ability to adapt to varying levels of input variance. It exhibits a robustness to noise and an ability to extract meaningful features from images, even under high levels of variance, in a manner reminiscent of \gls{V1} neurons recorded in Chapter 4. 
%This chapter also provides an opportunity to discuss the concept of a canonical intracortical variance computation repeated across different cortical areas. Drawing on the results from Chapter 4 and the model presented in this chapter, we argue that variance computation is purely a local computation. This suggests that each cortical area independently computes and encodes variance, contributing to a more robust and flexible representation of the visual world.

\item \textit{Chapter 5} builds upon the foundational concepts established previously to develop a model of graphs with a topology that depends on input variance. This is an extension of the type of neuronal recordings presented in Chapter 4, but with a significant shift in the data source, as the recordings here were carried using high-density matrices implanted in awake, behaving macaques from Pieter Roelfsema's group.
The fact that such results exists in two species of two different taxonomic genera confirms the relevance of the findings of Chapter 4 : recurrent interactions between neighboring V1 neurons are sufficient to explain the observed phenomena. Considering that the recordings presented in this chapter are preliminary and sourced from a single macaque, the content here is not developed into a full article and is notably shorter compared to other chapters.

\item \textit{Chapter 6} relates to the \textit{multiscale} aspect of this thesis. By diving into models of the subcortical pathways based on previous anatomical~\cite{abbas2020hierarchical} and functional studies~\cite{de2020pulvinar} of the laboratory, we first reveal the presence of alpha oscillation-gated activity in pulvinar, a critical component of predictive variance computations. This work sheds light on the intricate dynamics of the pulvinar, and its role in regulating the flow of information between visual cortical areas.
This is extended in a second contribution, which reviews the current literature of pulvinar as an information hub and describes the pulvinar as a regulator of (inverse) variance computations throughout the entire visual hierarchy. 

\item \textit{Chapter 7} briefly summarizes all the findings of previous chapters, revisiting key contributions of this thesis and highlighting their significance under the framework of predictive coding. We then explore potential future direction for further research, considering how these articles might be built upon and extended. Reflections upon the modes of investigation, their limit and the potential alternative approaches are also laid down. 
The broader impact of this work is also discussed in this chapter. It considers how these findings might influence the field of neuroscience at large, rather than being limited to visual neuroscience. It also contemplates the potential implications for related fields, such as clinical research and machine learning.

\item The \textit{Appendices} section provides derivations of equations pivotal to the conceptual framework discussed in this thesis. While the reading of this section is indeed optional, the formulation of these derivatives offer valuable insights into the foundational mathematics underlying the central concepts of the free energy principle. Additionally, the appendices include supplementary scientific material, such as press releases related to this research. 
\end{itemize}